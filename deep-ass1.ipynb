{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8292630,"sourceType":"datasetVersion","datasetId":4926375}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"6104d5b4-cb2a-42bb-8705-511875f30fe8","cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport pickle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5ba36ccf-935d-412c-ad7b-b2854458ef57","cell_type":"code","source":"# function for loading the training data:\n\ndef load_data(file):\n    \"\"\"\n    This function loads the data from the specified pickle file and returns a dictionary with the data\n    :param filename: the pickle file\n    :return: dict with data -- keys and values differ for the train data and test data for each task.\n     Please see the cells with example code below for explanations and examples of the data structure per data set.\n    \"\"\"\n    with open(file, 'rb') as f:\n        data_dict = pickle.load(f)\n    return data_dict","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"be9c1d84-dfe5-440f-9ffb-b838fac98cc1","cell_type":"code","source":"train_data = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/train_data.pkl')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"55a047d3-573b-4345-ba0e-7c5f328a675e","cell_type":"code","source":"type(train_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"85bde477-87e3-45a5-a19d-f9bd2117634f","cell_type":"code","source":"# the structure of the training data is a dict, where the keys are strings indicating the alphabet.\n# The values are again dicts, with the keys being the character and the values being a list of images of that character.\n\n# see the code below for examples of working with the train data\n\nalphabets = list(train_data.keys())\n\n\nprint('example alphabet names:', alphabets[:5])\nprint('\\n')\nprint('how to get an example image for a specific character:')\n\nalphabet_id = 4\nalphabet = alphabets[alphabet_id]  # a dict\ncharacters_for_this_alphabet = list(train_data[alphabet].keys())\ncharacter_id = 5\ncharacter = characters_for_this_alphabet[character_id]\nimage_id = 2\n\nprint(f'shape of image {image_id} of character {character} of alphabet {alphabet}:', train_data[alphabet][character][image_id].shape)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"57678ef4-2fb4-48d1-81ae-e4b4f78de9b1","cell_type":"code","source":"print(\"Loaded training data keys:\", train_data.keys())","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"a2d1d5e1-8924-4a0c-adc5-c5aa3e0ea380","cell_type":"code","source":"list(train_data.values())[2].keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3a468706-d73c-4938-8623-d4d122bb0e81","cell_type":"code","source":"np.array(list(list(train_data.values())[2].values())[0]).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a919886f-1204-4b83-93b3-3847bdd9bf45","cell_type":"code","source":"# function for plotting some examples:\n\ndef plot_example_data(data_dict):\n    \"\"\"\n    This function plots some examples of the data\n    :param data_dict: dict with as keys a string specifying the alphabet, and as values a dict with as keys the character of the alphabet, and as values a list om images of the alphabet\n    \"\"\"\n    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n    alphabets_to_plot = np.random.choice(list(data_dict.keys()), size=10, replace=False)\n    \n    for i, alphabet in enumerate(alphabets_to_plot):\n        characters = data_dict[alphabet]\n        character_to_plot = np.random.choice(list(characters.keys()), size=1)[0]\n        images = characters[character_to_plot]\n        im_idx = np.random.choice(len(images), size=1)[0]\n        axs[i//5, i%5].imshow(images[im_idx].permute(1, 2, 0))\n        axs[i//5, i%5].set_title(alphabet + '\\n' + character_to_plot, fontsize=8)\n        axs[i//5, i%5].axis('off')\n    # plt.show()\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"424e88a1-e7fb-4be6-b962-d65817326b6b","cell_type":"code","source":"plt.figure()\nplot_example_data(train_data)\n# plt.savefig('example_data.png', dpi=600)\nplt.show()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"99a2e60b-cc43-442b-8440-fd2b7be5c55f","cell_type":"markdown","source":"# Task 1: character recognition","metadata":{}},{"id":"4f48b2cc-ec49-49f7-90b5-b218f34e730a","cell_type":"code","source":"# our solution:","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"6455c713-fb2b-4999-a3eb-feeb716401b4","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8ab0428d-e578-4cb2-a3b7-f298207afe8f","cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport random\nimport torchvision.transforms as transforms\nimport numpy as np\n\n# Data Transforms\ndata_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ndef create_character_dataset(data_dict):\n    dataset = []\n    total_chars_so_far = 0\n    for alphabet, images in data_dict.items():\n        chars = list(images.keys())\n        for char, img_list in images.items():\n            label = total_chars_so_far + chars.index(char)\n            for img in img_list:\n                dataset.append((img, label))\n        total_chars_so_far += len(chars)\n    return dataset\n\nclass TripletCharacterDataset(Dataset):\n    def __init__(self, character_dataset, transform=None):\n        self.data = character_dataset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_anchor, label_anchor = self.data[idx]\n        \n        # Select a positive pair\n        while True:\n            img_positive, label_positive = random.choice(self.data)\n            if label_anchor == label_positive:\n                break\n        \n        # Select a negative pair\n        while True:\n            img_negative, label_negative = random.choice(self.data)\n            if label_anchor != label_negative:\n                break\n        \n        if self.transform:\n            img_anchor = self.transform(img_anchor)\n            img_positive = self.transform(img_positive)\n            img_negative = self.transform(img_negative)\n            \n        return img_anchor, img_positive, img_negative\n\n# Prepare the training dataset and dataloader\ntrain_dataset = TripletCharacterDataset(create_character_dataset(train_data), transform=data_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Inspect the dataset\nfor img_anchor, img_positive, img_negative in train_loader:\n    print(\"Batch of anchor images shape:\", img_anchor.shape)\n    print(\"Batch of positive images shape:\", img_positive.shape)\n    print(\"Batch of negative images shape:\", img_negative.shape)\n    break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7f95bc5e-605a-4f3d-a805-c88c1d2260b2","cell_type":"code","source":"def compute_top1_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            top1_idx = np.argmin(distances)\n            if annotated_labels[top1_idx] == unseen_labels[i]:\n                correct += 1\n    return correct / len(unseen_labels)\n\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task1.pkl')\ntest_alphabet = 'Mongolian'\nannotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][test_alphabet]])\nannotated_labels = data_dict_test['annotated_images_labels'][test_alphabet]\nunseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][test_alphabet]])\nunseen_labels = data_dict_test['unseen_images_labels'][test_alphabet]\n\n# Encode labels\nlabel_encoder = LabelEncoder()\nencoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\nencoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n# Convert to tensor\nannotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\nunseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n# Prepare test data for evaluation\ntest_data = (annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a3539099-3fcc-48ba-a5dd-6df87bfe05ee","cell_type":"code","source":"def initialize_weights(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.uniform_(m.weight.data)\n        if m.bias is not None:\n            nn.init.uniform_(m.bias.data)\n    elif isinstance(m, nn.Linear):\n        nn.init.uniform_(m.weight.data)\n        nn.init.uniform_(m.bias.data)\n\n# Define the Network\nclass CharacterNetwork(nn.Module):\n    def __init__(self):\n        super(CharacterNetwork, self).__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(),\n#             nn.MaxPool2d(2),\n#             nn.Conv2d(256, 512, 3, padding=1),\n#             nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32)  # Embedding size\n        )\n    \n    def forward(self, x):\n        x = self.cnn(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Triplet Loss\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n    \n    def forward(self, anchor, positive, negative):\n        positive_distance = F.pairwise_distance(anchor, positive)\n        negative_distance = F.pairwise_distance(anchor, negative)\n        loss = torch.mean(F.relu(positive_distance - negative_distance + self.margin))\n        return loss\n\n# Initialize the model, loss function, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CharacterNetwork().to(device)\nmodel.apply(initialize_weights)\n\ncriterion = TripletLoss(margin=1.0)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n\n# Function for training the model\ndef train_triplet(model, train_loader, criterion, optimizer, scheduler, num_epochs=10, test_data=None):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n        if epoch == int(num_epochs * 2 / 3):\n            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n        for img_anchor, img_positive, img_negative in progress_bar:\n            img_anchor, img_positive, img_negative = img_anchor.to(device), img_positive.to(device), img_negative.to(device)\n            \n            optimizer.zero_grad()\n            output_anchor = model(img_anchor)\n            output_positive = model(img_positive)\n            output_negative = model(img_negative)\n            loss = criterion(output_anchor, output_positive, output_negative)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * img_anchor.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n        scheduler.step(epoch_loss)\n        if test_data:\n            annotated_images, annotated_labels, unseen_images, unseen_labels = test_data\n            top1_accuracy = compute_top1_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels)\n            print(f'Top-1 Accuracy: {top1_accuracy:.4f}')\n\n        for param_group in optimizer.param_groups:\n            print(f'Learning Rate: {param_group[\"lr\"]}')\n\ntrain_triplet(model, train_loader, criterion, optimizer, scheduler, num_epochs=25, test_data=test_data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a12dcae5-3b9c-496e-80a8-8c5ea8cba8fd","cell_type":"code","source":"def compute_topk_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels, k=1):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            topk_idx = np.argsort(distances)[:k]\n            if unseen_labels[i] in annotated_labels[topk_idx]:\n                correct += 1\n    return correct / len(unseen_labels)\n\n# Load the test data\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task1.pkl')\ntest_data = []\nfor alphabet in data_dict_test['annotated_images'].keys():\n    annotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][alphabet]])\n    annotated_labels = data_dict_test['annotated_images_labels'][alphabet]\n    unseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][alphabet]])\n    unseen_labels = data_dict_test['unseen_images_labels'][alphabet]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\n    encoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n    # Convert to tensor\n    annotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\n    unseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n    # Prepare test data for evaluation\n    test_data.append((annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor))\n\nk_values = [1, 2, 4, 8]\naccuracies = {k: [] for k in k_values}\n\nfor data in test_data:\n    for k in k_values:\n        accuracies[k].append(compute_topk_accuracy(model, data[0], data[1], data[2], data[3], k=k))\n\nfor k in k_values:\n    print(f'Mean Top-{k} Accuracy: {np.mean(accuracies[k]):.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"38268324-a174-4b70-85e1-db2b98417011","cell_type":"code","source":"# example: let's get some annotated images and their labels for an alphabet in the test data:\n\nalphabets_test = list(data_dict_test['annotated_images'].keys())\nalphabet_id = np.random.randint(0, len(alphabets_test))\nalphabet = alphabets_test[alphabet_id]\n\nalphabet_annotated = data_dict_test['annotated_images'][alphabet]  # a tensor of shape (num_images, 1, height, width)\nprint(f'Shape of {alphabet} annotated images:', alphabet_annotated.shape)\n\nalphabet_annotated_labels = data_dict_test['annotated_images_labels'][alphabet]  # a list of length num_images\nprint(f'Number of {alphabet} annotated labels:', len(alphabet_annotated_labels))  # equals num_images\n\nalphabet_unseen = data_dict_test['unseen_images'][alphabet]  # a tensor of shape (num_images, 1, height, width)\nprint(f'Shape of {alphabet} unseen images:', alphabet_unseen.shape)\n\nalphabet_unseen_labels = data_dict_test['unseen_images_labels'][alphabet]  # a list of length num_images\nprint(f'Number of {alphabet} unseen labels: {len(alphabet_unseen_labels)}. Use the unseen labels only for evaluating your model!')  # equals num_images","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"207489af-632d-4e0f-a0c6-3f90f1778d8f","cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"1efc8e42-2257-4127-a673-5080d9aa290d","cell_type":"markdown","source":"# Task 2: rotation problem","metadata":{}},{"id":"42cb8d65-55ba-4a1f-9c5b-4f758f944798","cell_type":"code","source":"# function for plotting some examples:\n\ndef plot_example_data(data_dict):\n    \"\"\"\n    This function plots some examples of the data\n    :param data_dict: dict with as keys a string specifying the alphabet, and as values a dict with as keys the character of the alphabet, and as values a list om images of the alphabet\n    \"\"\"\n    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n    alphabets_to_plot = list(data_dict.keys())[:5]\n    \n    for i, alphabet in enumerate(alphabets_to_plot):\n        characters = data_dict[alphabet]\n#         character_to_plot = characters[:5]\n        images = characters[:5]\n        im_idx = np.random.choice(len(images), size=1)[0]\n        axs[i//5, i%5].imshow(images[0].permute(1, 2, 0))\n#         axs[i//5, i%5].set_title(alphabet + '\\n' + character_to_plot, fontsize=8)\n        axs[i//5, i%5].axis('off')\n    # plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"68ce3040-fcbb-47a0-92cb-2916985f5bef","cell_type":"code","source":"# load the test data for task 2:\n# the structure of the test data of task 2 is exactly the same as for task 1,\n# but now the images are rotated by an unknown angle between 0 and 360 degrees.\n# data_dict_test_task1 = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task1.pkl')\ndata_dict_test_task2 = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task2.pkl')\n\n# plot_example_data(data_dict_test_task1['annotated_images'])\nplot_example_data(data_dict_test_task2['annotated_images'])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"f0fa1427-4949-432d-b9ed-51ab13cef476","cell_type":"code","source":"data_dict_test_task2['annotated_images'].keys()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"32cb96d4-bbb8-48e3-89b8-6626b12e1195","cell_type":"code","source":"def compute_topk_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels, k=1):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            topk_idx = np.argsort(distances)[:k]\n            if unseen_labels[i] in annotated_labels[topk_idx]:\n                correct += 1\n    return correct / len(unseen_labels)\n\n# Load the test data\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task2.pkl')\ntest_data = []\nfor alphabet in data_dict_test['annotated_images'].keys():\n    annotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][alphabet]])\n    annotated_labels = data_dict_test['annotated_images_labels'][alphabet]\n    unseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][alphabet]])\n    unseen_labels = data_dict_test['unseen_images_labels'][alphabet]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\n    encoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n    # Convert to tensor\n    annotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\n    unseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n    # Prepare test data for evaluation\n    test_data.append((annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor))\n\nk_values = [1, 2, 4, 8]\naccuracies = {k: [] for k in k_values}\n\nfor data in test_data:\n    for k in k_values:\n        accuracies[k].append(compute_topk_accuracy(model, data[0], data[1], data[2], data[3], k=k))\n\nfor k in k_values:\n    print(f'Mean Top-{k} Accuracy: {np.mean(accuracies[k]):.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"749a0bb4-cc94-40ce-abb7-2d69f14f03ee","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport random\nimport torchvision.transforms as transforms\nimport numpy as np\n\n# Data Transforms with Rotation Only\ndata_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(90),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ndef create_character_dataset(data_dict):\n    dataset = []\n    total_chars_so_far = 0\n    for alphabet, images in data_dict.items():\n        chars = list(images.keys())\n        for char, img_list in images.items():\n            label = total_chars_so_far + chars.index(char)\n            for img in img_list:\n                dataset.append((img, label))\n        total_chars_so_far += len(chars)\n    return dataset\n\nclass TripletCharacterDataset(Dataset):\n    def __init__(self, character_dataset, transform=None):\n        self.data = character_dataset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_anchor, label_anchor = self.data[idx]\n        \n        # Select a positive pair\n        while True:\n            img_positive, label_positive = random.choice(self.data)\n            if label_anchor == label_positive:\n                break\n        \n        # Select a negative pair\n        while True:\n            img_negative, label_negative = random.choice(self.data)\n            if label_anchor != label_negative:\n                break\n        \n        if self.transform:\n            img_anchor = self.transform(img_anchor)\n            img_positive = self.transform(img_positive)\n            img_negative = self.transform(img_negative)\n        \n        return img_anchor, img_positive, img_negative\n\n# Prepare the training dataset and dataloader\ntrain_dataset = TripletCharacterDataset(create_character_dataset(train_data), transform=data_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Inspect the dataset\nfor img_anchor, img_positive, img_negative in train_loader:\n    print(\"Batch of anchor images shape:\", img_anchor.shape)\n    print(\"Batch of positive images shape:\", img_positive.shape)\n    print(\"Batch of negative images shape:\", img_negative.shape)\n    break\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"f16fe727-405c-4bd3-949f-f4aa6baa30e0","cell_type":"code","source":"def compute_top1_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            top1_idx = np.argmin(distances)\n            if annotated_labels[top1_idx] == unseen_labels[i]:\n                correct += 1\n    return correct / len(unseen_labels)\n\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task2.pkl')\ntest_alphabet = 'Mongolian'\nannotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][test_alphabet]])\nannotated_labels = data_dict_test['annotated_images_labels'][test_alphabet]\nunseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][test_alphabet]])\nunseen_labels = data_dict_test['unseen_images_labels'][test_alphabet]\n\n# Encode labels\nlabel_encoder = LabelEncoder()\nencoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\nencoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n# Convert to tensor\nannotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\nunseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n# Prepare test data for evaluation\nvalid_data = (annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4f2bce94-90ae-4bd5-89e8-f6da47a7808a","cell_type":"code","source":"def initialize_weights(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.uniform_(m.weight.data)\n        if m.bias is not None:\n            nn.init.uniform_(m.bias.data)\n    elif isinstance(m, nn.Linear):\n        nn.init.uniform_(m.weight.data)\n        nn.init.uniform_(m.bias.data)\n\n# Define the Network\nclass CharacterNetwork(nn.Module):\n    def __init__(self):\n        super(CharacterNetwork, self).__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=5, padding=0),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ZeroPad2d(1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 256, kernel_size=5, padding=0),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.3),\n\n            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 256, kernel_size=3, padding=0),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.3),\n\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 512, kernel_size=3, padding=0),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(0.3),\n\n            nn.AdaptiveAvgPool2d((1, 1)),\n\n            nn.Flatten(),\n            nn.Linear(512, 32),\n#             nn.BatchNorm1d(32),\n#             nn.Lambda(lambda x: F.normalize(x, p=2, dim=1))\n        )\n            \n    def forward(self, x):\n        x = self.cnn(x)\n        return x\n\n# Triplet Loss\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n    \n    def forward(self, anchor, positive, negative):\n        positive_distance = F.pairwise_distance(anchor, positive)\n        negative_distance = F.pairwise_distance(anchor, negative)\n        loss = torch.mean(F.relu(positive_distance - negative_distance + self.margin))\n        return loss\n\n# Initialize the model, loss function, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CharacterNetwork().to(device)\nmodel.apply(initialize_weights)\n\ncriterion = TripletLoss(margin=0.5)\noptimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)\n\n# Function for training the model\ndef train_triplet(model, train_loader, criterion, optimizer, scheduler, num_epochs=10, test_data=None):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n\n        for img_anchor, img_positive, img_negative in progress_bar:\n            img_anchor, img_positive, img_negative = img_anchor.to(device), img_positive.to(device), img_negative.to(device)\n            \n            optimizer.zero_grad()\n            output_anchor = model(img_anchor)\n            output_positive = model(img_positive)\n            output_negative = model(img_negative)\n            loss = criterion(output_anchor, output_positive, output_negative)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * img_anchor.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n        scheduler.step(epoch_loss)\n        if test_data:\n            annotated_images, annotated_labels, unseen_images, unseen_labels = test_data\n            top1_accuracy = compute_top1_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels)\n            print(f'Top-1 Accuracy: {top1_accuracy:.4f}')\n\n        for param_group in optimizer.param_groups:\n            print(f'Learning Rate: {param_group[\"lr\"]}')\n\ntrain_triplet(model, train_loader, criterion, optimizer, scheduler, num_epochs=40, test_data=valid_data)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"f10ba7c3-e394-4cea-93d1-1d9097c470ff","cell_type":"code","source":"data_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"3b5d9ece-7945-44ad-befe-9210c9485ae6","cell_type":"code","source":"\ndef compute_topk_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels, k=1):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            topk_idx = np.argsort(distances)[:k]\n            if unseen_labels[i] in annotated_labels[topk_idx]:\n                correct += 1\n    return correct / len(unseen_labels)\n\n# Load the test data\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task2.pkl')\ntest_data = []\nfor alphabet in data_dict_test['annotated_images'].keys():\n    annotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][alphabet]])\n    annotated_labels = data_dict_test['annotated_images_labels'][alphabet]\n    unseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][alphabet]])\n    unseen_labels = data_dict_test['unseen_images_labels'][alphabet]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\n    encoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n    # Convert to tensor\n    annotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\n    unseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n    # Prepare test data for evaluation\n    test_data.append((annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor))\n\nk_values = [1, 2, 4, 8]\naccuracies = {k: [] for k in k_values}\n\nfor data in test_data:\n    for k in k_values:\n        accuracies[k].append(compute_topk_accuracy(model, data[0], data[1], data[2], data[3], k=k))\n\nfor k in k_values:\n    print(f'Mean Top-{k} Accuracy: {np.mean(accuracies[k]):.4f}')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"40f57609-b87d-4d8e-8fc8-5ca4d7b9c9fe","cell_type":"code","source":"def compute_topk_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels, k=1):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            topk_idx = np.argsort(distances)[:k]\n            if unseen_labels[i] in annotated_labels[topk_idx]:\n                correct += 1\n    return correct / len(unseen_labels)\n\n# Load the test data\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task1.pkl')\ntest_data = []\nfor alphabet in data_dict_test['annotated_images'].keys():\n    annotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][alphabet]])\n    annotated_labels = data_dict_test['annotated_images_labels'][alphabet]\n    unseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][alphabet]])\n    unseen_labels = data_dict_test['unseen_images_labels'][alphabet]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\n    encoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n    # Convert to tensor\n    annotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\n    unseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n    # Prepare test data for evaluation\n    test_data.append((annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor))\n\nk_values = [1, 2, 4, 8]\naccuracies = {k: [] for k in k_values}\n\nfor data in test_data:\n    for k in k_values:\n        accuracies[k].append(compute_topk_accuracy(model, data[0], data[1], data[2], data[3], k=k))\n\nfor k in k_values:\n    print(f'Mean Top-{k} Accuracy: {np.mean(accuracies[k]):.4f}')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"da089d3f-e6a6-4a6e-8656-a64afd5ac349","cell_type":"markdown","source":"# Task 3: Domain knowledge injection","metadata":{}},{"id":"91ed4add-bed0-4ad4-a28b-4114bf0355b4","cell_type":"code","source":"# load the test data for task 3:\n# the structure of the data of task 3 is exactly the same as for task 1, but now our the loaded dictionary contains some additional keys.\n# These additional keys will be explained in the cells below:\n\ndata_dict_test_task3 = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task3.pkl')\nprint(data_dict_test_task3.keys())","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"f7b342d1-2f4b-4382-83ab-a1d841054c3a","cell_type":"code","source":"data_dict_test_task3['unseen_images_preceding_types']['Angelic'][:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"60f0656c-d1da-4cdd-86c4-5c228962402b","cell_type":"code","source":"data_dict_test_task3['character_to_type_mapping']['Angelic']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cd0ec79f-e9bd-44e3-b442-8f7c26acdf17","cell_type":"code","source":"data_dict_test_task3['type_following_probs']['Angelic']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"669a915e-997a-452e-8530-b8842b72bf11","cell_type":"code","source":"# The keys 'annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels' are the same as for task 1, and the structure of the data is exactly the same. \n\n# The key 'unseen_images_preceding_types' maps to the type of the preceding character in the sequence where the unseen image was observed, for each alphabet.\n# The key 'character_to_type_mapping' maps to the mapping of each character to its type, for each alphabet.\n# The key 'type_following_probs' maps to the probabilities of each character type being followed by another character type, for each alphabet.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"5378c1c3-0dd9-4ba5-b76a-f6cfeab40b76","cell_type":"code","source":"# examples:\n\nalphabet = np.random.choice(list(data_dict_test_task3['unseen_images_preceding_types'].keys()))\nprint(f'Alphabet: {alphabet}')\n\n\npreceding_character_types_alphabet = data_dict_test_task3[\"unseen_images_preceding_types\"][alphabet]  # a list\nprint(f'Some character types that preceded unseen images from the {alphabet} alphabet: {np.random.choice(preceding_character_types_alphabet, size=5)}')\nprint(f'There are {len(preceding_character_types_alphabet)} preceding character types in the {alphabet} alphabet, and {len(data_dict_test_task3[\"unseen_images\"][alphabet])} unseen images.')\n\n\ncharacter_to_type_mapping_alphabet = data_dict_test_task3[\"character_to_type_mapping\"][alphabet]  \n# this is a dict, with as keys the characters and as values the types\nrandom_character = np.random.choice(list(character_to_type_mapping_alphabet.keys()))\nprint(f'Type of {random_character} from the {alphabet} alphabet: {character_to_type_mapping_alphabet[random_character]}')\n\n\n\ntype_following_probs_alphabet = data_dict_test_task3[\"type_following_probs\"][alphabet]  # a dict of dicts\npreceding_type = np.random.choice(list(type_following_probs_alphabet.keys()))\nfollowing_type = np.random.choice(list(type_following_probs_alphabet[preceding_type].keys()))\nprint(f'Probability of a character of type {following_type} following a character of type {preceding_type} in the {alphabet} alphabet: {type_following_probs_alphabet[preceding_type][following_type]}')\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"7112921e-21cb-4136-8089-22232c5a1232","cell_type":"code","source":"data_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\ndef compute_topk_accuracy(model, annotated_images, annotated_labels, unseen_images, unseen_labels, k=1):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        annotated_embeddings = model(annotated_images.to(device)).cpu().numpy()\n        unseen_embeddings = model(unseen_images.to(device)).cpu().numpy()\n        for i, unseen_embedding in enumerate(unseen_embeddings):\n            distances = np.linalg.norm(annotated_embeddings - unseen_embedding, axis=1)\n            topk_idx = np.argsort(distances)[:k]\n            if unseen_labels[i] in annotated_labels[topk_idx]:\n                correct += 1\n    return correct / len(unseen_labels)\n\n# Load the test data\ndata_dict_test = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task3.pkl')\ntest_data = []\nfor alphabet in data_dict_test['annotated_images'].keys():\n    annotated_images = torch.stack([data_transforms(img) for img in data_dict_test['annotated_images'][alphabet]])\n    annotated_labels = data_dict_test['annotated_images_labels'][alphabet]\n    unseen_images = torch.stack([data_transforms(img) for img in data_dict_test['unseen_images'][alphabet]])\n    unseen_labels = data_dict_test['unseen_images_labels'][alphabet]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\n    encoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n    # Convert to tensor\n    annotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\n    unseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n    # Prepare test data for evaluation\n    test_data.append((annotated_images, annotated_labels_tensor, unseen_images, unseen_labels_tensor))\n\nk_values = [1, 2, 4, 8]\naccuracies = {k: [] for k in k_values}\n\nfor data in test_data:\n    for k in k_values:\n        accuracies[k].append(compute_topk_accuracy(model, data[0], data[1], data[2], data[3], k=k))\n\nfor k in k_values:\n    print(f'Mean Top-{k} Accuracy: {np.mean(accuracies[k]):.4f}')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"8ff007df-8f52-4817-8633-54f9c505de21","cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\n\n# Function to generate embeddings\ndef generate_embeddings(model, images, device):\n    model.eval()\n    with torch.no_grad():\n        embeddings = model(images.to(device)).cpu().numpy()\n    return embeddings\n\n# Function to adjust distances based on type-following probabilities\ndef adjust_distances(distances, annotated_labels, unseen_types, character_to_type, type_following_probs, label_encoder):\n    adjusted_distances = distances.copy()\n    for i, unseen_type in enumerate(unseen_types):\n        for j, annotated_label in enumerate(annotated_labels):\n            character_label = label_encoder.inverse_transform([annotated_label])[0]\n            annotated_type = character_to_type[character_label]\n            prob = type_following_probs[unseen_type][annotated_type]\n            adjusted_distances[i, j] *= (1 - prob)  # Adjusting distances inversely proportional to the probability\n    \n    return adjusted_distances\n\n# Function to classify unseen images with type-following probabilities\ndef classify_unseen_images_with_probs(model, annotated_images, annotated_labels, unseen_images, unseen_types, character_to_type, type_following_probs, label_encoder, k=1):\n    annotated_embeddings = generate_embeddings(model, annotated_images, device)\n    unseen_embeddings = generate_embeddings(model, unseen_images, device)\n    \n    distances = euclidean_distances(unseen_embeddings, annotated_embeddings)\n    adjusted_distances = adjust_distances(distances, annotated_labels, unseen_types, character_to_type, type_following_probs, label_encoder)\n    \n    top_k_indices = adjusted_distances.argsort(axis=1)[:, :k]\n    top_k_predictions = np.array([[annotated_labels[idx] for idx in indices] for indices in top_k_indices])\n    \n    return top_k_predictions\n\n# Function to calculate top-k accuracy\ndef top_k_accuracy(true_labels, top_k_predictions, k):\n    correct = 0\n    for true_label, pred_labels in zip(true_labels, top_k_predictions):\n        if true_label in pred_labels[:k]:\n            correct += 1\n    return correct / len(true_labels)\n\n# Load the test data for task 3\ndata_dict_test_task3 = load_data('/kaggle/input/data-deep/data_A1_2AMM10_2023_2024/test_data_task3.pkl')\n\n# Initialize variables to store results\nk_values = [1, 2, 4, 8]\ntop_k_accuracies = {k: [] for k in k_values}\n\n# Loop through each alphabet in the test data\nfor alphabet in (data_dict_test_task3['annotated_images'].keys()):\n    annotated_images = torch.stack([data_transforms(img) for img in data_dict_test_task3['annotated_images'][alphabet]])\n    annotated_labels = data_dict_test_task3['annotated_images_labels'][alphabet]\n    unseen_images = torch.stack([data_transforms(img) for img in data_dict_test_task3['unseen_images'][alphabet]])\n    unseen_labels = data_dict_test_task3['unseen_images_labels'][alphabet]\n    unseen_types = data_dict_test_task3['unseen_images_preceding_types'][alphabet]\n    character_to_type = data_dict_test_task3['character_to_type_mapping'][alphabet]\n    type_following_probs = data_dict_test_task3['type_following_probs'][alphabet]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_annotated_labels = label_encoder.fit_transform(annotated_labels)\n    encoded_unseen_labels = label_encoder.transform(unseen_labels)\n\n    # Convert to tensor\n    annotated_labels_tensor = torch.tensor(encoded_annotated_labels, dtype=torch.long)\n    unseen_labels_tensor = torch.tensor(encoded_unseen_labels, dtype=torch.long)\n\n    # Prepare test data for evaluation\n    for k in k_values:\n        top_k_predictions = classify_unseen_images_with_probs(model, annotated_images, annotated_labels_tensor, unseen_images, unseen_types, character_to_type, type_following_probs, label_encoder, k=k)\n        accuracy_k = top_k_accuracy(unseen_labels_tensor.numpy(), top_k_predictions, k=k)\n        top_k_accuracies[k].append(accuracy_k)\n\n# Compute mean top-k accuracy\nfor k in k_values:\n    mean_accuracy_k = np.mean(top_k_accuracies[k])\n    print(f'Mean Top-{k} Accuracy: {mean_accuracy_k:.4f}')\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"910546cb-ab64-465d-a169-7dbe1885337c","cell_type":"code","source":"torch.save(model.state_dict(), 'model.pt')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"6e51a273-005b-4ec8-b5f9-2388d1b8a5fe","cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"id":"1d781cff-4b1c-4555-8fab-1b29f7de8370","cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null}]}